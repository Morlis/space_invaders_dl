{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f7f6c945a6bb0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Deep Q-Learning - Space Invaders\n",
    "Train an agent to play Space Invaders using [Deep Reinforcement Learning](https://deepmind.google/discover/blog/deep-reinforcement-learning/)\n",
    "\n",
    "# Outline\n",
    "- [ 1 - Import Packages <img align=\"Right\" src=\"./images/space_invaders.png\" width = 60% >](#1)\n",
    "- [ 2 - Hyperparameters](#2)\n",
    "- [ 3 - The Game (Space Invaders)](#3)\n",
    "  - [ 3.1 Action Space](#3.1)\n",
    "  - [ 3.2 Observation Space](#3.2)\n",
    "  - [ 3.3 Rewards](#3.3)\n",
    "  - [ 3.4 Episode Termination](#3.4)\n",
    "- [ 4 - Load the game using Selenium](#4)\n",
    "- [ 5 - Game State](#5)\n",
    "- [ 6 - Deep Q-Learning](#6)\n",
    "  - [ 6.1 Target Network](#6.1)\n",
    "  - [ 6.2 Experience Replay](#6.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53e16a6c69a299",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Import Packages\n",
    "\n",
    "We'll make use of the following packages:\n",
    "- `selenium` is a package for Selenium Python bindings provides a simple API to write functional/acceptance tests using Selenium WebDriver.\n",
    "- `PIL` is a package from Pillow, the Python Imaging Library adds image processing capabilities.\n",
    "- `OpenCV` (Open Source Computer Vision Library) is an open source computer vision and machine learning software library.\n",
    "- `numpy` is the fundamental package for scientific computing in Python.\n",
    "- `TensorFlow` is an end-to-end open source platform for machine learning\n",
    "- `TensorFlow.Keras` is the high-level API of the TensorFlow platform. It provides an approachable, highly-productive interface for solving machine learning (ML) problems, with a focus on modern deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be470be0b73af651",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "582871b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from config import Config\n",
    "from classes.game import Game\n",
    "from classes.game_state import Game_State\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1690dc792b362",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Hyperparameters\n",
    "\n",
    "Agent hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bb076542d7732",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate\n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2c49d",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Space Invaders\n",
    "In this notebook we will be using [free80sarcade Space Invaders](https://www.free80sarcade.com/spaceinvaders.php) web version.\n",
    "\n",
    "Our goal is develop an AI agent that is able to learn how to play the popular game Space Invaders from scratch. To do it, we implement a Deep Reinforcement Learning algorithm using both Keras on top of Tensorflow.\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Action Space\n",
    "\n",
    "The agent has four discrete actions available:\n",
    "\n",
    "* Do nothing.\n",
    "* Move right.\n",
    "* Move left.\n",
    "* Fire weapon.\n",
    "\n",
    "Each action has a corresponding numerical value:\n",
    "\n",
    "```python\n",
    "Do nothing = 0\n",
    "Fire right engine = 1\n",
    "Fire main engine = 2\n",
    "Fire left engine = 3\n",
    "```\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Observation Space\n",
    "\n",
    "<img src=\"./images/observation_space.png\" width=\"268\" height=\"158\" ><br />\n",
    "\n",
    "Screen coordinate: x = 0, y = 58, cx = 268, cy = 158\n",
    "\n",
    "BW Image size: 268 * 158 = 42344\n",
    "\n",
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Rewards\n",
    "\n",
    "After every step, a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "- is increased by the difference between the previous and current scores.\n",
    "- is decreased by 0.03 each time the weapon is fired\n",
    "- is decreased by 100 when a live is lost\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Episode Termination\n",
    "\n",
    "An episode ends (i.e the environment enters a terminal state) if:\n",
    "\n",
    "* The number of lives reach 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0fd1b",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Load Space Invaders using Selenium\n",
    "\n",
    "We start by loading the `Space Invaders` emulation from the [free80sarcade](https://www.free80sarcade.com/spaceinvaders.php) website by using the `Selenium` webdriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29869f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# Create an instance of the Service object\n",
    "service = Service(executable_path=config.chrome_driver)\n",
    "# Start Chrome using the service keyword\n",
    "driver = webdriver.Chrome(service=service)\n",
    "# Load the site\n",
    "driver.get(config.game_url)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Cookie consent\n",
    "wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[class='fc-button fc-cta-consent fc-primary-button']\"))).click()\n",
    "\n",
    "# Find canvas and create Game\n",
    "canvas = wait.until(EC.element_to_be_clickable((By.ID, 'videoCanvas')))\n",
    "\n",
    "# Create the game\n",
    "game = Game(driver, canvas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01f2fa",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Game State\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d3ac5e",
   "metadata": {},
   "source": [
    "To build our neural network later on we need to know the size of the state vector and the number of valid actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c51d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (158, 268)\n",
      "Number of actions: 3\n"
     ]
    }
   ],
   "source": [
    "state_size = Game_State.shape\n",
    "num_actions = Game_State.num_actions\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45ec3a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0\n",
      "Lives: 3\n"
     ]
    }
   ],
   "source": [
    "game.start()\n",
    "\n",
    "game_state = Game_State(game.get_image_bw())\n",
    "\n",
    "cv2.imshow('Gray', game_state.state())\n",
    "cv2.waitKey()\n",
    "\n",
    "print(f\"Score: %s\" % game_state.score())\n",
    "print(f\"Lives: %s\" % game_state.lives())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f52df",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389cd29",
   "metadata": {},
   "source": [
    "<a name=\"6.1\"></a>\n",
    "### 6.1 Target Network\n",
    "\n",
    "We can train the $Q$-Network by adjusting it's weights at each iteration to minimize the mean-squared error in the Bellman equation, where the target values are given by:\n",
    "\n",
    "$$\n",
    "y = R + \\gamma \\max_{a'}Q(s',a';w)\n",
    "$$\n",
    "\n",
    "where $w$ are the weights of the $Q$-Network. This means that we are adjusting the weights $w$ at each iteration to minimize the following error:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "Notice that this forms a problem because the $y$ target is changing on every iteration. Having a constantly moving target can lead to oscillations and instabilities. To avoid this, we can create a separate neural network for generating the $y$ targets. \n",
    "\n",
    "We call this separate neural network the **target $\\hat Q$-Network** and it will have the same architecture as the original $Q$-Network. By using the target $\\hat Q$-Network, the above error becomes:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "where $w^-$ and $w$ are the weights of the target $\\hat Q$-Network and $Q$-Network, respectively.\n",
    "\n",
    "In practice, we will use the following algorithm: every $C$ time steps we will use the $\\hat Q$-Network to generate the $y$ targets and update the weights of the target $\\hat Q$-Network using the weights of the $Q$-Network. We will update the weights $w^-$ of the the target $\\hat Q$-Network using a **soft update**. This means that we will update the weights $w^-$ using the following rule:\n",
    " \n",
    "$$\n",
    "w^-\\leftarrow \\tau w + (1 - \\tau) w^-\n",
    "$$\n",
    "\n",
    "where $\\tau\\ll 1$. By using the soft update, we are ensuring that the target values, $y$, change slowly, which greatly improves the stability of our learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b35734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "    Input(shape=state_size), \n",
    "    Dense(units=64, activation='relu'), \n",
    "    Dense(units=64, activation='relu'), \n",
    "    Dense(units=num_actions, activation='linear')\n",
    "    ])\n",
    "\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),                       \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=64, activation='relu'),            \n",
    "    Dense(units=num_actions, activation='linear')\n",
    "    ])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.number_parser import Number_Parser\n",
    "import cv2\n",
    "import random as rng\n",
    "\n",
    "# 9,9\n",
    "# Lives 21, 218\n",
    "# Score 37, 42 - 45 - 53 - 61 - 69 - 77\n",
    "\n",
    "#matrix = np.asarray(bw_image.getdata(0)).reshape((bw_image.size[1], bw_image.size[0]))\n",
    "\n",
    "# Convert image to BGR2GRAY format needed by findContours\n",
    "gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "score_digits = [None] * 6\n",
    "score_digits[0] = gray[42:42 + 9, 37:37 + 9]\n",
    "score_digits[1] = gray[42:42 + 9, 45:45 + 9]\n",
    "score_digits[2] = gray[42:42 + 9, 53:53 + 9]\n",
    "score_digits[3] = gray[42:42 + 9, 61:61 + 9]\n",
    "score_digits[4] = gray[42:42 + 9, 69:69 + 9]\n",
    "score_digits[5] = gray[42:42 + 9, 77:77 + 9]\n",
    "\n",
    "cv2.imshow('Gray', score_digits[0])\n",
    "cv2.waitKey()\n",
    "\n",
    "#print(model.predict(cv2.bitwise_not(score_digits[0])))\n",
    "#print(model.predict(cv2.bitwise_not(cv2.resize(score_digits[1], (200, 200)))))\n",
    "#print(model.predict(cv2.bitwise_not(score_digits[2])))\n",
    "#print(model.predict(cv2.bitwise_not(score_digits[3])))\n",
    "\n",
    "number_parser = Number_Parser()\n",
    "\n",
    "print(number_parser.get_number(score_digits))\n",
    "#print(number_parser.get_digit(score_digits[1]))\n",
    "\n",
    "\"\"\"\n",
    "number_contours = pickle.load(open('number_contours.dump', 'rb'))\n",
    "number_contours[0] = score_digits[0]\n",
    "number_contours[1] = score_digits[1]\n",
    "number_contours[2] = score_digits[2]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "digit_contours, _ = cv2.findContours(score_digits[0], cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "number_contours[0] = digit_contours[0]\n",
    "\n",
    "digit_contours, _ = cv2.findContours(score_digits[1], cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "number_contours[1] = digit_contours[0]\n",
    "\n",
    "digit_contours, _ = cv2.findContours(score_digits[2], cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "number_contours[2] = digit_contours[0]\n",
    "\"\"\"\n",
    "\n",
    "#pickle.dump(number_contours, open('number_contours.dump', 'wb'))\n",
    "\n",
    "#ret = cv2.matchShapes(score_digits[0], score_digits[3], 1, 0.0)\n",
    "#print(ret)\n",
    "\n",
    "#cv2.imshow('Gray', score_digits[2])\n",
    "#cv2.waitKey()\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(gray, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "contours_poly = [None]*len(contours)\n",
    "boundRect = [None]*len(contours)\n",
    "centers = [None]*len(contours)\n",
    "radius = [None]*len(contours)\n",
    "for i, c in enumerate(contours):\n",
    "    contours_poly[i] = cv2.approxPolyDP(c, 3, True)\n",
    "    boundRect[i] = cv2.boundingRect(contours_poly[i])\n",
    "    centers[i], radius[i] = cv2.minEnclosingCircle(contours_poly[i])\n",
    "\n",
    "drawing = np.zeros((gray.shape[0], gray.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(len(contours)):\n",
    "    color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "    cv2.drawContours(drawing, contours_poly, i, color)\n",
    "    #cv2.rectangle(drawing, (int(boundRect[i][0]), int(boundRect[i][1])), \\\n",
    "    #   (int(boundRect[i][0]+boundRect[i][2]), int(boundRect[i][1]+boundRect[i][3])), color, 2)\n",
    "    #cv2.circle(drawing, (int(centers[i][0]), int(centers[i][1])), int(radius[i]), color, 2)\n",
    "\n",
    "cv2.imshow('Gray', drawing)\n",
    "cv2.waitKey()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.number_parser import Number_Parser\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "max_num_timesteps = 100\n",
    "\n",
    "number_parser = Number_Parser()\n",
    "game.start()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "for t in range(max_num_timesteps):\n",
    "    image = game.get_image_png()\n",
    "\n",
    "    # Convert image to BGR2GRAY format needed by findContours\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    score_digits = [None] * 6\n",
    "    score_digits[0] = gray[42:42 + 9, 37:37 + 9]\n",
    "    score_digits[1] = gray[42:42 + 9, 45:45 + 9]\n",
    "    score_digits[2] = gray[42:42 + 9, 53:53 + 9]\n",
    "    score_digits[3] = gray[42:42 + 9, 61:61 + 9]\n",
    "    score_digits[4] = gray[42:42 + 9, 69:69 + 9]\n",
    "    score_digits[5] = gray[42:42 + 9, 77:77 + 9]\n",
    "\n",
    "    lives_digit = gray[218:218 + 9, 21:21 + 9]\n",
    "\n",
    "    print(f\"Score: %s\" % number_parser.get_number(score_digits))\n",
    "    print(f\"Lives: %s\" % number_parser.get_digit(lives_digit))\n",
    "    print(\"----------\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.fire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0aa03088",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b374337",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
