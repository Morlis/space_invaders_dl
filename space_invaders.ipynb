{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f7f6c945a6bb0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Deep Q-Learning - Space Invaders\n",
    "Train an agent to play Space Invaders using [Deep Reinforcement Learning](https://deepmind.google/discover/blog/deep-reinforcement-learning/)\n",
    "\n",
    "# Outline\n",
    "- [ 1 - Import Packages <img align=\"Right\" src=\"./images/space_invaders.png\" width = 60% >](#1)\n",
    "- [ 2 - Hyperparameters](#2)\n",
    "- [ 3 - The Game (Space Invaders)](#3)\n",
    "  - [ 3.1 Action Space](#3.1)\n",
    "  - [ 3.2 Observation Space](#3.2)\n",
    "  - [ 3.3 Rewards](#3.3)\n",
    "  - [ 3.4 Episode Termination](#3.4)\n",
    "- [ 4 - Load the game using Selenium](#4)\n",
    "- [ 5 - Game State](#5)\n",
    "- [ 6 - Deep Q-Learning](#6)\n",
    "  - [ 6.1 Target Network](#6.1)\n",
    "  - [ 6.2 Experience Replay](#6.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53e16a6c69a299",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Import Packages\n",
    "\n",
    "We'll make use of the following packages:\n",
    "- `selenium` is a package for Selenium Python bindings provides a simple API to write functional/acceptance tests using Selenium WebDriver.\n",
    "- `PIL` is a package from Pillow, the Python Imaging Library adds image processing capabilities.\n",
    "- `OpenCV` (Open Source Computer Vision Library) is an open source computer vision and machine learning software library.\n",
    "- `numpy` is the fundamental package for scientific computing in Python.\n",
    "- `TensorFlow` is an end-to-end open source platform for machine learning\n",
    "- `TensorFlow.Keras` is the high-level API of the TensorFlow platform. It provides an approachable, highly-productive interface for solving machine learning (ML) problems, with a focus on modern deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be470be0b73af651",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "582871b0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T14:40:13.791169Z",
     "start_time": "2024-02-19T14:40:13.742826Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from config import Config\n",
    "from classes.game import Game\n",
    "from classes.game_state import Game_State\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1690dc792b362",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Hyperparameters\n",
    "\n",
    "Agent hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562bb076542d7732",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T14:34:06.452026Z",
     "start_time": "2024-02-19T14:34:06.381249Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate\n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2c49d",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Space Invaders\n",
    "In this notebook we will be using [free80sarcade Space Invaders](https://www.free80sarcade.com/spaceinvaders.php) web version.\n",
    "\n",
    "Our goal is develop an AI agent that is able to learn how to play the popular game Space Invaders from scratch. To do it, we implement a Deep Reinforcement Learning algorithm using both Keras on top of Tensorflow.\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Action Space\n",
    "\n",
    "The agent has four discrete actions available:\n",
    "\n",
    "* Do nothing.\n",
    "* Move right.\n",
    "* Move left.\n",
    "* Fire weapon.\n",
    "\n",
    "Each action has a corresponding numerical value:\n",
    "\n",
    "```python\n",
    "Do nothing = 0\n",
    "Fire right engine = 1\n",
    "Fire main engine = 2\n",
    "Fire left engine = 3\n",
    "```\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Observation Space\n",
    "\n",
    "<img src=\"./images/observation_space.png\" width=\"268\" height=\"158\" ><br />\n",
    "\n",
    "Screen coordinate: x = 0, y = 58, cx = 268, cy = 158\n",
    "\n",
    "BW Image size: 268 * 158 = 42344\n",
    "\n",
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Rewards\n",
    "\n",
    "After every step, a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "- is increased by the difference between the previous and current scores.\n",
    "- is decreased by 0.03 each time the weapon is fired\n",
    "- is decreased by 100 when a live is lost\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Episode Termination\n",
    "\n",
    "An episode ends (i.e the environment enters a terminal state) if:\n",
    "\n",
    "* The number of lives reach 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0fd1b",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Load Space Invaders using Selenium\n",
    "\n",
    "We start by loading the `Space Invaders` emulation from the [free80sarcade](https://www.free80sarcade.com/spaceinvaders.php) website by using the `Selenium` webdriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29869f47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:35:50.795820Z",
     "start_time": "2024-02-19T14:34:21.626228Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# Create an instance of the Service object\n",
    "service = Service(executable_path=config.chrome_driver)\n",
    "# Start Chrome using the service keyword\n",
    "driver = webdriver.Chrome(service=service)\n",
    "# Load the site\n",
    "driver.get(config.game_url)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Cookie consent\n",
    "wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[class='fc-button fc-cta-consent fc-primary-button']\"))).click()\n",
    "\n",
    "# Find canvas and create Game\n",
    "canvas = wait.until(EC.element_to_be_clickable((By.ID, 'videoCanvas')))\n",
    "\n",
    "# Create the game\n",
    "game = Game(driver, canvas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01f2fa",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Game State\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d3ac5e",
   "metadata": {},
   "source": [
    "To build our neural network later on we need to know the size of the state vector and the number of valid actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c51d4a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T09:59:15.768209Z",
     "start_time": "2024-02-19T09:59:15.524885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape: (158, 268)\n",
      "Number of actions: 3\n"
     ]
    }
   ],
   "source": [
    "state_size = Game_State.shape\n",
    "num_actions = Game_State.num_actions\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ec3a84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T09:59:54.459208Z",
     "start_time": "2024-02-19T09:59:19.269234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0\n",
      "Lives: 0\n"
     ]
    }
   ],
   "source": [
    "game.start()\n",
    "\n",
    "game_state = Game_State(game.get_image_bw())\n",
    "\n",
    "cv2.imshow('Gray', game_state.state())\n",
    "cv2.waitKey()\n",
    "\n",
    "print(f\"Score: %s\" % game_state.score())\n",
    "print(f\"Lives: %s\" % game_state.lives())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389cd29",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Deep Q-Learning\n",
    "\n",
    "<a name=\"6.1\"></a>\n",
    "### 6.1 Target Network\n",
    "\n",
    "We can train the $Q$-Network by adjusting it's weights at each iteration to minimize the mean-squared error in the Bellman equation, where the target values are given by:\n",
    "\n",
    "$$\n",
    "y = R + \\gamma \\max_{a'}Q(s',a';w)\n",
    "$$\n",
    "\n",
    "where $w$ are the weights of the $Q$-Network. This means that we are adjusting the weights $w$ at each iteration to minimize the following error:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "Notice that this forms a problem because the $y$ target is changing on every iteration. Having a constantly moving target can lead to oscillations and instabilities. To avoid this, we can create a separate neural network for generating the $y$ targets.\n",
    "\n",
    "We call this separate neural network the **target $\\hat Q$-Network** and it will have the same architecture as the original $Q$-Network. By using the target $\\hat Q$-Network, the above error becomes:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "where $w^-$ and $w$ are the weights of the target $\\hat Q$-Network and $Q$-Network, respectively.\n",
    "\n",
    "In practice, we will use the following algorithm: every $C$ time steps we will use the $\\hat Q$-Network to generate the $y$ targets and update the weights of the target $\\hat Q$-Network using the weights of the $Q$-Network. We will update the weights $w^-$ of the the target $\\hat Q$-Network using a **soft update**. This means that we will update the weights $w^-$ using the following rule:\n",
    "\n",
    "$$\n",
    "w^-\\leftarrow \\tau w + (1 - \\tau) w^-\n",
    "$$\n",
    "\n",
    "where $\\tau\\ll 1$. By using the soft update, we are ensuring that the target values, $y$, change slowly, which greatly improves the stability of our learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b35734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=num_actions, activation='linear')\n",
    "    ])\n",
    "\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=num_actions, activation='linear')\n",
    "    ])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"6.2\"></a>\n",
    "### 6.2 Experience Replay\n",
    "\n",
    "When an agent interacts with the environment, the states, actions, and rewards the agent experiences are sequential by nature. If the agent tries to learn from these consecutive experiences it can run into problems due to the strong correlations between them. To avoid this, we employ a technique known as **Experience Replay** to generate uncorrelated experiences for training our agent. Experience replay consists of storing the agent's experiences (i.e the states, actions, and rewards the agent receives) in a memory buffer and then sampling a random mini-batch of experiences from the buffer to do the learning. The experience tuples $(S_t, A_t, R_t, S_{t+1})$ will be added to the memory buffer at each time step as the agent interacts with the environment.\n",
    "\n",
    "For convenience, we will store the experiences as named tuples."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38e5412e9e2a2a45"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:14.146586Z",
     "start_time": "2024-02-19T14:41:14.104801Z"
    }
   },
   "id": "4ff113924a543c0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.number_parser import Number_Parser\n",
    "import cv2\n",
    "import random as rng\n",
    "\n",
    "# 9,9\n",
    "# Lives 21, 218\n",
    "# Score 37, 42 - 45 - 53 - 61 - 69 - 77\n",
    "\n",
    "#matrix = np.asarray(bw_image.getdata(0)).reshape((bw_image.size[1], bw_image.size[0]))\n",
    "\n",
    "# Convert image to BGR2GRAY format needed by findContours\n",
    "gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "score_digits = [None] * 6\n",
    "score_digits[0] = gray[42:42 + 9, 37:37 + 9]\n",
    "score_digits[1] = gray[42:42 + 9, 45:45 + 9]\n",
    "score_digits[2] = gray[42:42 + 9, 53:53 + 9]\n",
    "score_digits[3] = gray[42:42 + 9, 61:61 + 9]\n",
    "score_digits[4] = gray[42:42 + 9, 69:69 + 9]\n",
    "score_digits[5] = gray[42:42 + 9, 77:77 + 9]\n",
    "\n",
    "cv2.imshow('Gray', score_digits[0])\n",
    "cv2.waitKey()\n",
    "\n",
    "#print(model.predict(cv2.bitwise_not(score_digits[0])))\n",
    "#print(model.predict(cv2.bitwise_not(cv2.resize(score_digits[1], (200, 200)))))\n",
    "#print(model.predict(cv2.bitwise_not(score_digits[2])))\n",
    "#print(model.predict(cv2.bitwise_not(score_digits[3])))\n",
    "\n",
    "number_parser = Number_Parser()\n",
    "\n",
    "print(number_parser.get_number(score_digits))\n",
    "#print(number_parser.get_digit(score_digits[1]))\n",
    "\n",
    "\"\"\"\n",
    "number_contours = pickle.load(open('number_contours.dump', 'rb'))\n",
    "number_contours[0] = score_digits[0]\n",
    "number_contours[1] = score_digits[1]\n",
    "number_contours[2] = score_digits[2]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "digit_contours, _ = cv2.findContours(score_digits[0], cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "number_contours[0] = digit_contours[0]\n",
    "\n",
    "digit_contours, _ = cv2.findContours(score_digits[1], cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "number_contours[1] = digit_contours[0]\n",
    "\n",
    "digit_contours, _ = cv2.findContours(score_digits[2], cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "number_contours[2] = digit_contours[0]\n",
    "\"\"\"\n",
    "\n",
    "#pickle.dump(number_contours, open('number_contours.dump', 'wb'))\n",
    "\n",
    "#ret = cv2.matchShapes(score_digits[0], score_digits[3], 1, 0.0)\n",
    "#print(ret)\n",
    "\n",
    "#cv2.imshow('Gray', score_digits[2])\n",
    "#cv2.waitKey()\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(gray, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "contours_poly = [None]*len(contours)\n",
    "boundRect = [None]*len(contours)\n",
    "centers = [None]*len(contours)\n",
    "radius = [None]*len(contours)\n",
    "for i, c in enumerate(contours):\n",
    "    contours_poly[i] = cv2.approxPolyDP(c, 3, True)\n",
    "    boundRect[i] = cv2.boundingRect(contours_poly[i])\n",
    "    centers[i], radius[i] = cv2.minEnclosingCircle(contours_poly[i])\n",
    "\n",
    "drawing = np.zeros((gray.shape[0], gray.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(len(contours)):\n",
    "    color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "    cv2.drawContours(drawing, contours_poly, i, color)\n",
    "    #cv2.rectangle(drawing, (int(boundRect[i][0]), int(boundRect[i][1])), \\\n",
    "    #   (int(boundRect[i][0]+boundRect[i][2]), int(boundRect[i][1]+boundRect[i][3])), color, 2)\n",
    "    #cv2.circle(drawing, (int(centers[i][0]), int(centers[i][1])), int(radius[i]), color, 2)\n",
    "\n",
    "cv2.imshow('Gray', drawing)\n",
    "cv2.waitKey()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.number_parser import Number_Parser\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "max_num_timesteps = 100\n",
    "\n",
    "number_parser = Number_Parser()\n",
    "game.start()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "for t in range(max_num_timesteps):\n",
    "    image = game.get_image_png()\n",
    "\n",
    "    # Convert image to BGR2GRAY format needed by findContours\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    score_digits = [None] * 6\n",
    "    score_digits[0] = gray[42:42 + 9, 37:37 + 9]\n",
    "    score_digits[1] = gray[42:42 + 9, 45:45 + 9]\n",
    "    score_digits[2] = gray[42:42 + 9, 53:53 + 9]\n",
    "    score_digits[3] = gray[42:42 + 9, 61:61 + 9]\n",
    "    score_digits[4] = gray[42:42 + 9, 69:69 + 9]\n",
    "    score_digits[5] = gray[42:42 + 9, 77:77 + 9]\n",
    "\n",
    "    lives_digit = gray[218:218 + 9, 21:21 + 9]\n",
    "\n",
    "    print(f\"Score: %s\" % number_parser.get_number(score_digits))\n",
    "    print(f\"Lives: %s\" % number_parser.get_digit(lives_digit))\n",
    "    print(\"----------\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.fire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0aa03088",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b374337",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
