{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f7f6c945a6bb0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Deep Q-Learning - Space Invaders\n",
    "Train an agent to play Space Invaders using [Deep Reinforcement Learning](https://deepmind.google/discover/blog/deep-reinforcement-learning/)\n",
    "\n",
    "# Outline\n",
    "- [ 1 - Import Packages <img align=\"Right\" src=\"./images/space_invaders.png\" width = 60% >](#1)\n",
    "- [ 2 - Hyperparameters](#2)\n",
    "- [ 3 - The Game (Space Invaders)](#3)\n",
    "  - [ 3.1 Action Space](#3.1)\n",
    "  - [ 3.2 Observation Space](#3.2)\n",
    "  - [ 3.3 Rewards](#3.3)\n",
    "  - [ 3.4 Episode Termination](#3.4)\n",
    "- [ 4 - Load the game using Selenium](#4)\n",
    "- [ 5 - Game State](#5)\n",
    "- [ 6 - Deep Q-Learning](#6)\n",
    "  - [ 6.1 Target Network](#6.1)\n",
    "  - [ 6.2 Experience Replay](#6.2)\n",
    "- [ 7 - Deep Q-Learning Algorithm with Experience Replay](#7)\n",
    "- [ 8 - Update the Network Weights](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53e16a6c69a299",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Import Packages\n",
    "\n",
    "We'll make use of the following packages:\n",
    "- `selenium` is a package for Selenium Python bindings provides a simple API to write functional/acceptance tests using Selenium WebDriver.\n",
    "- `PIL` is a package from Pillow, the Python Imaging Library adds image processing capabilities.\n",
    "- `OpenCV` (Open Source Computer Vision Library) is an open source computer vision and machine learning software library.\n",
    "- `numpy` is the fundamental package for scientific computing in Python.\n",
    "- `TensorFlow` is an end-to-end open source platform for machine learning\n",
    "- `TensorFlow.Keras` is the high-level API of the TensorFlow platform. It provides an approachable, highly-productive interface for solving machine learning (ML) problems, with a focus on modern deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be470be0b73af651",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582871b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "from config import Config\n",
    "from classes.game import Game\n",
    "from classes.game_state import Game_State\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1690dc792b362",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Hyperparameters\n",
    "\n",
    "Agent hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bb076542d7732",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate\n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2c49d",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Space Invaders\n",
    "In this notebook we will be using [free80sarcade Space Invaders](https://www.free80sarcade.com/spaceinvaders.php) web version.\n",
    "\n",
    "Our goal is develop an AI agent that is able to learn how to play the popular game Space Invaders from scratch. To do it, we implement a Deep Reinforcement Learning algorithm using both Keras on top of Tensorflow.\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Action Space\n",
    "\n",
    "The agent has four discrete actions available:\n",
    "\n",
    "* Do nothing.\n",
    "* Move right.\n",
    "* Move left.\n",
    "* Fire weapon.\n",
    "\n",
    "Each action has a corresponding numerical value:\n",
    "\n",
    "```python\n",
    "Do nothing = 0\n",
    "Fire right engine = 1\n",
    "Fire main engine = 2\n",
    "Fire left engine = 3\n",
    "```\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Observation Space\n",
    "\n",
    "<img src=\"./images/observation_space.png\" width=\"268\" height=\"158\" ><br />\n",
    "\n",
    "Screen coordinate: x = 0, y = 58, cx = 268, cy = 158\n",
    "\n",
    "BW Image size: 268 * 158 = 42344\n",
    "\n",
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Rewards\n",
    "\n",
    "After every step, a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "- is increased by the difference between the previous and current scores.\n",
    "- is decreased by 0.03 each time the weapon is fired\n",
    "- is decreased by 100 when a live is lost\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Episode Termination\n",
    "\n",
    "An episode ends (i.e the environment enters a terminal state) if:\n",
    "\n",
    "* The number of lives reach 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0fd1b",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Load Space Invaders using Selenium\n",
    "\n",
    "We start by loading the `Space Invaders` emulation from the [free80sarcade](https://www.free80sarcade.com/spaceinvaders.php) website by using the `Selenium` webdriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29869f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# Create an instance of the Service object\n",
    "service = Service(executable_path=config.chrome_driver)\n",
    "# Start Chrome using the service keyword\n",
    "driver = webdriver.Chrome(service=service)\n",
    "# Load the site\n",
    "driver.get(config.game_url)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Cookie consent\n",
    "wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[class='fc-button fc-cta-consent fc-primary-button']\"))).click()\n",
    "\n",
    "# Find canvas and create Game\n",
    "canvas = wait.until(EC.element_to_be_clickable((By.ID, 'videoCanvas')))\n",
    "\n",
    "# Create the game\n",
    "game = Game(driver, canvas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01f2fa",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Game State\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d3ac5e",
   "metadata": {},
   "source": [
    "To build our neural network later on we need to know the size of the state vector and the number of valid actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = Game_State.shape\n",
    "num_actions = Game_State.num_actions\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.start()\n",
    "\n",
    "game_state = Game_State(game.get_image_bw())\n",
    "\n",
    "cv2.imshow('Gray', game_state.state())\n",
    "cv2.waitKey()\n",
    "\n",
    "print(f\"Score: %s\" % game_state.score())\n",
    "print(f\"Lives: %s\" % game_state.lives())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389cd29",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Deep Q-Learning\n",
    "\n",
    "<a name=\"6.1\"></a>\n",
    "### 6.1 Target Network\n",
    "\n",
    "We can train the $Q$-Network by adjusting it's weights at each iteration to minimize the mean-squared error in the Bellman equation, where the target values are given by:\n",
    "\n",
    "$$\n",
    "y = R + \\gamma \\max_{a'}Q(s',a';w)\n",
    "$$\n",
    "\n",
    "where $w$ are the weights of the $Q$-Network. This means that we are adjusting the weights $w$ at each iteration to minimize the following error:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "Notice that this forms a problem because the $y$ target is changing on every iteration. Having a constantly moving target can lead to oscillations and instabilities. To avoid this, we can create a separate neural network for generating the $y$ targets.\n",
    "\n",
    "We call this separate neural network the **target $\\hat Q$-Network** and it will have the same architecture as the original $Q$-Network. By using the target $\\hat Q$-Network, the above error becomes:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "where $w^-$ and $w$ are the weights of the target $\\hat Q$-Network and $Q$-Network, respectively.\n",
    "\n",
    "In practice, we will use the following algorithm: every $C$ time steps we will use the $\\hat Q$-Network to generate the $y$ targets and update the weights of the target $\\hat Q$-Network using the weights of the $Q$-Network. We will update the weights $w^-$ of the the target $\\hat Q$-Network using a **soft update**. This means that we will update the weights $w^-$ using the following rule:\n",
    "\n",
    "$$\n",
    "w^-\\leftarrow \\tau w + (1 - \\tau) w^-\n",
    "$$\n",
    "\n",
    "where $\\tau\\ll 1$. By using the soft update, we are ensuring that the target values, $y$, change slowly, which greatly improves the stability of our learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b35734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=num_actions, activation='linear')\n",
    "    ])\n",
    "\n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=num_actions, activation='linear')\n",
    "    ])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5412e9e2a2a45",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a name=\"6.2\"></a>\n",
    "### 6.2 Experience Replay\n",
    "\n",
    "When an agent interacts with the environment, the states, actions, and rewards the agent experiences are sequential by nature. If the agent tries to learn from these consecutive experiences it can run into problems due to the strong correlations between them. To avoid this, we employ a technique known as **Experience Replay** to generate uncorrelated experiences for training our agent. Experience replay consists of storing the agent's experiences (i.e the states, actions, and rewards the agent receives) in a memory buffer and then sampling a random mini-batch of experiences from the buffer to do the learning. The experience tuples $(S_t, A_t, R_t, S_{t+1})$ will be added to the memory buffer at each time step as the agent interacts with the environment.\n",
    "\n",
    "For convenience, we will store the experiences as named tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff113924a543c0b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "By using experience replay we avoid problematic correlations, oscillations and instabilities. In addition, experience replay also allows the agent to potentially use the same experience in multiple weight updates, which increases data efficiency."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bbc9aef520f2ede"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"7\"></a>\n",
    "## 7 - Deep Q-Learning Algorithm with Experience Replay\n",
    "\n",
    "Now we will compute the loss between the $y$ targets and the $Q(s,a)$ values.\n",
    "The `compute_loss` function will set the $y$ targets equal to:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y_j =\n",
    "    \\begin{cases}\n",
    "      R_j & \\text{if episode terminates at step  } j+1\\\\\n",
    "      R_j + \\gamma \\max_{a'}\\hat{Q}(s_{j+1},a') & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here are a couple of things to note:\n",
    "\n",
    "* The `compute_loss` function takes in a mini-batch of experience tuples. This mini-batch of experience tuples is unpacked to extract the `states`, `actions`, `rewards`, `next_states`, and `done_vals`. You should keep in mind that these variables are *TensorFlow Tensors* whose size will depend on the mini-batch size. For example, if the mini-batch size is `64` then both `rewards` and `done_vals` will be TensorFlow Tensors with `64` elements.\n",
    "\n",
    "* Using `if/else` statements to set the $y$ targets will not work when the variables are tensors with many elements. However, notice that we can use the `done_vals` to implement the above in a single line of code. To do this, recall that the `done` variable is a Boolean variable that takes the value `True` when an episode terminates at step $j+1$ and it is `False` otherwise. Taking into account that a Boolean value of `True` has the numerical value of `1` and a Boolean value of `False` has the numerical value of `0`, we can use the factor `(1 - done_vals)` to implement the above in a single line of code.\n",
    "\n",
    "Lastly, we compute the loss by calculating the Mean-Squared Error (`MSE`) between the `y_targets` and the `q_values`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3446b19c9ccd8a70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\"\n",
    "    Calculates the loss.\n",
    "\n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "\n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "\n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "\n",
    "    # Set y = R if episode terminates, otherwise set y = R + Î³ max Q^(s,a).\n",
    "    y_targets = rewards + (gamma * max_qsa * (1 - done_vals))\n",
    "\n",
    "    # Get the q_values and reshape to match y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = MSE(y_targets, q_values)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.number_parser import Number_Parser\n",
    "import cv2\n",
    "import random as rng\n",
    "\n",
    "# 9,9\n",
    "# Lives 21, 218\n",
    "# Score 37, 42 - 45 - 53 - 61 - 69 - 77\n",
    "\n",
    "#matrix = np.asarray(bw_image.getdata(0)).reshape((bw_image.size[1], bw_image.size[0]))\n",
    "\n",
    "# Convert image to BGR2GRAY format needed by findContours\n",
    "gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "score_digits = [None] * 6\n",
    "score_digits[0] = gray[42:42 + 9, 37:37 + 9]\n",
    "score_digits[1] = gray[42:42 + 9, 45:45 + 9]\n",
    "score_digits[2] = gray[42:42 + 9, 53:53 + 9]\n",
    "score_digits[3] = gray[42:42 + 9, 61:61 + 9]\n",
    "score_digits[4] = gray[42:42 + 9, 69:69 + 9]\n",
    "score_digits[5] = gray[42:42 + 9, 77:77 + 9]\n",
    "\n",
    "cv2.imshow('Gray', score_digits[0])\n",
    "cv2.waitKey()\n",
    "\n",
    "#print(model.predict(cv2.bitwise_not(score_digits[0])))\n",
    "#print(model.predict(cv2.bitwise_not(cv2.resize(score_digits[1], (200, 200)))))\n",
    "#print(model.predict(cv2.bitwise_not(score_digits[2])))\n",
    "#print(model.predict(cv2.bitwise_not(score_digits[3])))\n",
    "\n",
    "number_parser = Number_Parser()\n",
    "\n",
    "print(number_parser.get_number(score_digits))\n",
    "#print(number_parser.get_digit(score_digits[1]))\n",
    "\n",
    "\"\"\"\n",
    "number_contours = pickle.load(open('number_contours.dump', 'rb'))\n",
    "number_contours[0] = score_digits[0]\n",
    "number_contours[1] = score_digits[1]\n",
    "number_contours[2] = score_digits[2]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "digit_contours, _ = cv2.findContours(score_digits[0], cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "number_contours[0] = digit_contours[0]\n",
    "\n",
    "digit_contours, _ = cv2.findContours(score_digits[1], cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "number_contours[1] = digit_contours[0]\n",
    "\n",
    "digit_contours, _ = cv2.findContours(score_digits[2], cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "number_contours[2] = digit_contours[0]\n",
    "\"\"\"\n",
    "\n",
    "#pickle.dump(number_contours, open('number_contours.dump', 'wb'))\n",
    "\n",
    "#ret = cv2.matchShapes(score_digits[0], score_digits[3], 1, 0.0)\n",
    "#print(ret)\n",
    "\n",
    "#cv2.imshow('Gray', score_digits[2])\n",
    "#cv2.waitKey()\n",
    "\n",
    "# Find contours\n",
    "contours, _ = cv2.findContours(gray, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "contours_poly = [None]*len(contours)\n",
    "boundRect = [None]*len(contours)\n",
    "centers = [None]*len(contours)\n",
    "radius = [None]*len(contours)\n",
    "for i, c in enumerate(contours):\n",
    "    contours_poly[i] = cv2.approxPolyDP(c, 3, True)\n",
    "    boundRect[i] = cv2.boundingRect(contours_poly[i])\n",
    "    centers[i], radius[i] = cv2.minEnclosingCircle(contours_poly[i])\n",
    "\n",
    "drawing = np.zeros((gray.shape[0], gray.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(len(contours)):\n",
    "    color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "    cv2.drawContours(drawing, contours_poly, i, color)\n",
    "    #cv2.rectangle(drawing, (int(boundRect[i][0]), int(boundRect[i][1])), \\\n",
    "    #   (int(boundRect[i][0]+boundRect[i][2]), int(boundRect[i][1]+boundRect[i][3])), color, 2)\n",
    "    #cv2.circle(drawing, (int(centers[i][0]), int(centers[i][1])), int(radius[i]), color, 2)\n",
    "\n",
    "cv2.imshow('Gray', drawing)\n",
    "cv2.waitKey()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.number_parser import Number_Parser\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "max_num_timesteps = 100\n",
    "\n",
    "number_parser = Number_Parser()\n",
    "game.start()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "for t in range(max_num_timesteps):\n",
    "    image = game.get_image_png()\n",
    "\n",
    "    # Convert image to BGR2GRAY format needed by findContours\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    score_digits = [None] * 6\n",
    "    score_digits[0] = gray[42:42 + 9, 37:37 + 9]\n",
    "    score_digits[1] = gray[42:42 + 9, 45:45 + 9]\n",
    "    score_digits[2] = gray[42:42 + 9, 53:53 + 9]\n",
    "    score_digits[3] = gray[42:42 + 9, 61:61 + 9]\n",
    "    score_digits[4] = gray[42:42 + 9, 69:69 + 9]\n",
    "    score_digits[5] = gray[42:42 + 9, 77:77 + 9]\n",
    "\n",
    "    lives_digit = gray[218:218 + 9, 21:21 + 9]\n",
    "\n",
    "    print(f\"Score: %s\" % number_parser.get_number(score_digits))\n",
    "    print(f\"Lives: %s\" % number_parser.get_digit(lives_digit))\n",
    "    print(\"----------\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.fire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa03088",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b374337",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
